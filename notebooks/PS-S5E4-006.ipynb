{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a91f4b2a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T11:36:02.575275Z",
     "iopub.status.busy": "2025-04-21T11:36:02.574911Z",
     "iopub.status.idle": "2025-04-21T11:36:10.304722Z",
     "shell.execute_reply": "2025-04-21T11:36:10.303678Z"
    },
    "papermill": {
     "duration": 7.736183,
     "end_time": "2025-04-21T11:36:10.306244",
     "exception": false,
     "start_time": "2025-04-21T11:36:02.570061",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m13.5/13.5 MB\u001b[0m \u001b[31m97.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "category-encoders 2.7.0 requires scikit-learn<1.6.0,>=1.0.0, but you have scikit-learn 1.6.1 which is incompatible.\r\n",
      "bigframes 1.36.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -qq scikit-learn==1.6.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66093433",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-04-21T11:36:10.313432Z",
     "iopub.status.busy": "2025-04-21T11:36:10.313144Z",
     "iopub.status.idle": "2025-04-21T11:36:10.320694Z",
     "shell.execute_reply": "2025-04-21T11:36:10.319928Z"
    },
    "papermill": {
     "duration": 0.012182,
     "end_time": "2025-04-21T11:36:10.321717",
     "exception": false,
     "start_time": "2025-04-21T11:36:10.309535",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class CFG:\n",
    "    train_path: Path = Path(\"/kaggle/input/playground-series-s5e4/train.csv\")\n",
    "    test_path: Path = Path(\"/kaggle/input/playground-series-s5e4/test.csv\")\n",
    "    sub_path: Path = Path(\"/kaggle/input/playground-series-s5e4/sample_submission.csv\")\n",
    "    pltpd_path: Path = Path(\"/kaggle/input/podcast-listening-time-prediction-dataset/podcast_dataset.csv\")\n",
    "    \n",
    "    num_fold: int = 5\n",
    "    dev_mode: bool = False\n",
    "\n",
    "    # Model parameters\n",
    "    n_iter: int = 10000\n",
    "    max_depth: int = -1\n",
    "    num_leaves: int = 1024\n",
    "    colsample_bytree: float = 0.7\n",
    "    learning_rate: float = 0.04\n",
    "\n",
    "    objective: str = \"l2\"\n",
    "    metric: str = \"rmse\"\n",
    "    verbosity: int = -1\n",
    "\n",
    "    random_state: int = 42\n",
    "    shuffle: bool = True\n",
    "    encoded_columns_start: int = -91\n",
    "    log_eval: int = 100\n",
    "    early_stopping: int = 200\n",
    "\n",
    "    # debug = True\n",
    "\n",
    "\n",
    "cfg = CFG()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3387d95e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T11:36:10.328263Z",
     "iopub.status.busy": "2025-04-21T11:36:10.328003Z",
     "iopub.status.idle": "2025-04-21T11:36:13.902040Z",
     "shell.execute_reply": "2025-04-21T11:36:13.901463Z"
    },
    "papermill": {
     "duration": 3.579183,
     "end_time": "2025-04-21T11:36:13.903633",
     "exception": false,
     "start_time": "2025-04-21T11:36:10.324450",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "from itertools import combinations\n",
    "\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "def calc_rmse(y_true, y_pred):\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    return rmse\n",
    "\n",
    "\n",
    "re_dict = {}\n",
    "re_dict[\"podc_dict\"] = {\n",
    "    \"Mystery Matters\": 0,\n",
    "    \"Joke Junction\": 1,\n",
    "    \"Study Sessions\": 2,\n",
    "    \"Digital Digest\": 3,\n",
    "    \"Mind & Body\": 4,\n",
    "    \"Fitness First\": 5,\n",
    "    \"Criminal Minds\": 6,\n",
    "    \"News Roundup\": 7,\n",
    "    \"Daily Digest\": 8,\n",
    "    \"Music Matters\": 9,\n",
    "    \"Sports Central\": 10,\n",
    "    \"Melody Mix\": 11,\n",
    "    \"Game Day\": 12,\n",
    "    \"Gadget Geek\": 13,\n",
    "    \"Global News\": 14,\n",
    "    \"Tech Talks\": 15,\n",
    "    \"Sport Spot\": 16,\n",
    "    \"Funny Folks\": 17,\n",
    "    \"Sports Weekly\": 18,\n",
    "    \"Business Briefs\": 19,\n",
    "    \"Tech Trends\": 20,\n",
    "    \"Innovators\": 21,\n",
    "    \"Health Hour\": 22,\n",
    "    \"Comedy Corner\": 23,\n",
    "    \"Sound Waves\": 24,\n",
    "    \"Brain Boost\": 25,\n",
    "    \"Athlete's Arena\": 26,\n",
    "    \"Wellness Wave\": 27,\n",
    "    \"Style Guide\": 28,\n",
    "    \"World Watch\": 29,\n",
    "    \"Humor Hub\": 30,\n",
    "    \"Money Matters\": 31,\n",
    "    \"Healthy Living\": 32,\n",
    "    \"Home & Living\": 33,\n",
    "    \"Educational Nuggets\": 34,\n",
    "    \"Market Masters\": 35,\n",
    "    \"Learning Lab\": 36,\n",
    "    \"Lifestyle Lounge\": 37,\n",
    "    \"Crime Chronicles\": 38,\n",
    "    \"Detective Diaries\": 39,\n",
    "    \"Life Lessons\": 40,\n",
    "    \"Current Affairs\": 41,\n",
    "    \"Finance Focus\": 42,\n",
    "    \"Laugh Line\": 43,\n",
    "    \"True Crime Stories\": 44,\n",
    "    \"Business Insights\": 45,\n",
    "    \"Fashion Forward\": 46,\n",
    "    \"Tune Time\": 47,\n",
    "}\n",
    "re_dict[\"genr_dict\"] = {\n",
    "    \"True Crime\": 0,\n",
    "    \"Comedy\": 1,\n",
    "    \"Education\": 2,\n",
    "    \"Technology\": 3,\n",
    "    \"Health\": 4,\n",
    "    \"News\": 5,\n",
    "    \"Music\": 6,\n",
    "    \"Sports\": 7,\n",
    "    \"Business\": 8,\n",
    "    \"Lifestyle\": 9,\n",
    "}\n",
    "re_dict[\"week_dict\"] = {\n",
    "    \"Monday\": 0,\n",
    "    \"Tuesday\": 1,\n",
    "    \"Wednesday\": 2,\n",
    "    \"Thursday\": 3,\n",
    "    \"Friday\": 4,\n",
    "    \"Saturday\": 5,\n",
    "    \"Sunday\": 6,\n",
    "}\n",
    "re_dict[\"time_dict\"] = {\"Morning\": 10, \"Afternoon\": 14, \"Evening\": 17, \"Night\": 21}\n",
    "re_dict[\"sent_dict\"] = {\"Negative\": 0, \"Neutral\": 1, \"Positive\": 2}\n",
    "\n",
    "\n",
    "pl_i_type = pl.Int32\n",
    "pl_f_type = pl.Float32\n",
    "\n",
    "\n",
    "def cast_numeric_dtypes(df):\n",
    "    float_cols = [col for col in df.columns if df.schema[col] == pl.Float64 or df.schema[col] == pl.Float32]\n",
    "    int_cols = [col for col in df.columns if df.schema[col] == pl.Int64 or df.schema[col] == pl.Int32]\n",
    "\n",
    "    if float_cols:\n",
    "        df = df.with_columns([pl.col(col).cast(pl_f_type) for col in float_cols])\n",
    "    if int_cols:\n",
    "        df = df.with_columns([pl.col(col).cast(pl_i_type) for col in int_cols])\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def preprocess(df, df_train=None):\n",
    "    df = cast_numeric_dtypes(df)\n",
    "    df = df.with_columns(pl.col(\"Episode_Title\").str.slice(8).cast(pl.Int32).alias(\"Episode_Num\")).drop(\"Episode_Title\")\n",
    "\n",
    "    # Convert categorical variables using mapping\n",
    "    for col, mapping in [\n",
    "        (\"Genre\", re_dict[\"genr_dict\"]),\n",
    "        (\"Podcast_Name\", re_dict[\"podc_dict\"]),\n",
    "        (\"Publication_Day\", re_dict[\"week_dict\"]),\n",
    "        (\"Publication_Time\", re_dict[\"time_dict\"]),\n",
    "        (\"Episode_Sentiment\", re_dict[\"sent_dict\"]),\n",
    "    ]:\n",
    "        df = df.with_columns(pl.col(col).replace(mapping).alias(col))\n",
    "\n",
    "    # Cap extreme values\n",
    "    df = df.with_columns(\n",
    "        pl.when(pl.col(\"Episode_Length_minutes\") > 121.0).then(121.0).otherwise(pl.col(\"Episode_Length_minutes\")).alias(\"Episode_Length_minutes\"),\n",
    "        pl.when(pl.col(\"Number_of_Ads\") > 103.91).then(103.91).otherwise(pl.col(\"Number_of_Ads\")).alias(\"Number_of_Ads\"),\n",
    "    )\n",
    "\n",
    "    # Create NaN indicator columns\n",
    "    df = df.with_columns(\n",
    "        pl.col(\"Episode_Length_minutes\").is_null().cast(pl.Utf8).cast(pl.Categorical).alias(\"Episode_Length_minutes_NaN\"),\n",
    "        pl.col(\"Guest_Popularity_percentage\").is_null().cast(pl.Utf8).cast(pl.Categorical).alias(\"Guest_Popularity_percentage_NaN\"),\n",
    "        pl.col(\"Number_of_Ads\").is_null().cast(pl.Utf8).cast(pl.Categorical).alias(\"Number_of_Ads_NaN\"),\n",
    "    )\n",
    "\n",
    "    # Fill NA values with median\n",
    "    if df_train is None:\n",
    "        df_train = df.clone()\n",
    "\n",
    "    e_median = df_train.select(pl.col(\"Episode_Length_minutes\").median()).item()\n",
    "    g_median = df_train.select(pl.col(\"Guest_Popularity_percentage\").median()).item()\n",
    "    n_median = df_train.select(pl.col(\"Number_of_Ads\").median()).item()\n",
    "\n",
    "    df = df.with_columns(\n",
    "        pl.col(\"Episode_Length_minutes\").fill_null(e_median),\n",
    "        pl.col(\"Guest_Popularity_percentage\").fill_null(g_median),\n",
    "        pl.col(\"Number_of_Ads\").fill_null(n_median),\n",
    "    )\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def feature_eng(df, df_train):\n",
    "    # Cyclical features for day and time\n",
    "    df = df.with_columns(\n",
    "        # Day features\n",
    "        pl.col(\"Publication_Day\").cast(pl_f_type).mul(2 * np.pi / 7).sin().alias(\"Day_sin\"),\n",
    "        pl.col(\"Publication_Day\").cast(pl_f_type).mul(2 * np.pi / 7).cos().alias(\"Day_cos\"),\n",
    "        pl.col(\"Publication_Day\").cast(pl_f_type).mul(4 * np.pi / 7).sin().alias(\"Day_sin2\"),\n",
    "        pl.col(\"Publication_Day\").cast(pl_f_type).mul(4 * np.pi / 7).cos().alias(\"Day_cos2\"),\n",
    "        # Time features\n",
    "        pl.col(\"Publication_Time\").cast(pl_f_type).mul(2 * np.pi / 4).sin().alias(\"Time_sin\"),\n",
    "        pl.col(\"Publication_Time\").cast(pl_f_type).mul(2 * np.pi / 4).cos().alias(\"Time_cos\"),\n",
    "        pl.col(\"Publication_Time\").cast(pl_f_type).mul(4 * np.pi / 24).sin().alias(\"Time_sin2\"),\n",
    "        pl.col(\"Publication_Time\").cast(pl_f_type).mul(4 * np.pi / 24).cos().alias(\"Time_cos2\"),\n",
    "        # Ratio features\n",
    "        (pl.col(\"Episode_Length_minutes\") / (pl.col(\"Number_of_Ads\") + 1)).fill_null(0).alias(\"Length_per_Ads\"),\n",
    "        (pl.col(\"Episode_Length_minutes\") / (pl.col(\"Host_Popularity_percentage\") + 1)).fill_null(0).alias(\"Length_per_Host\"),\n",
    "        (pl.col(\"Episode_Length_minutes\") / (pl.col(\"Guest_Popularity_percentage\") + 1)).fill_null(0).alias(\"Length_per_Guest\"),\n",
    "        # Episode length features\n",
    "        pl.col(\"Episode_Length_minutes\").floor().alias(\"ELen_Int\"),\n",
    "        (pl.col(\"Episode_Length_minutes\") - pl.col(\"Episode_Length_minutes\").floor()).alias(\"ELen_Dec\"),\n",
    "        pl.col(\"Host_Popularity_percentage\").floor().alias(\"HPperc_Int\"),\n",
    "        (pl.col(\"Host_Popularity_percentage\") - pl.col(\"Host_Popularity_percentage\").floor()).alias(\"HPperc_Dec\"),\n",
    "        # Sentiment features\n",
    "        (pl.col(\"Episode_Sentiment\") == \"2\").cast(pl.Int8).alias(\"Is_Positive_Sentiment\"),\n",
    "        pl.when(pl.col(\"Episode_Sentiment\") == \"2\").then(0.75).otherwise(0.717).cast(pl_f_type).alias(\"Sentiment_Multiplier\"),\n",
    "        # Squared features\n",
    "        (pl.col(\"Episode_Length_minutes\") ** 2).alias(\"Episode_Length_squared\"),\n",
    "        (pl.col(\"Episode_Length_minutes\") ** 3).alias(\"Episode_Length_squared2\"),\n",
    "    )\n",
    "\n",
    "    # Add expected listening time based on sentiment\n",
    "    df = df.with_columns((pl.col(\"Episode_Length_minutes\") * pl.col(\"Sentiment_Multiplier\")).alias(\"Expected_Listening_Time_Sentiment\"))\n",
    "\n",
    "    # Convert columns to categorical\n",
    "    for col in [\"Podcast_Name\", \"Genre\", \"Publication_Day\", \"Publication_Time\", \"Episode_Sentiment\", \"Episode_Num\"]:\n",
    "        df = df.with_columns(pl.col(col).cast(pl.Utf8).cast(pl.Categorical))\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def cols_encode(df):\n",
    "    # Order of importance\n",
    "    columns_to_encode = [\n",
    "        \"Host_Popularity_percentage\",\n",
    "        \"Guest_Popularity_percentage\",\n",
    "        \"Episode_Length_minutes\",\n",
    "        \"Episode_Num\",\n",
    "        \"Podcast_Name\",\n",
    "        \"Publication_Day\",\n",
    "        \"Publication_Time\",\n",
    "        # \"Episode_Sentiment\",\n",
    "        \"Genre\",\n",
    "        \"Number_of_Ads\",\n",
    "        \"Episode_Length_minutes_NaN\",\n",
    "        \"Guest_Popularity_percentage_NaN\",\n",
    "        \"HPperc_Int\",\n",
    "        \"HPperc_Dec\",\n",
    "        \"ELen_Int\",\n",
    "        \"ELen_Dec\",\n",
    "        \"Length_per_Ads\",\n",
    "    ]\n",
    "\n",
    "    pair_size = [2]\n",
    "\n",
    "    # Pure polars implementation for combinations\n",
    "    for r in pair_size:\n",
    "        combinations_list = list(combinations(columns_to_encode, r))\n",
    "        batch_size = 20\n",
    "\n",
    "        print(\"\\n pair_size:\", r, \"\\n\")\n",
    "\n",
    "        for i in range(0, len(combinations_list), batch_size):\n",
    "            batch = combinations_list[i : i + batch_size]\n",
    "\n",
    "            for cols in tqdm(batch):\n",
    "                new_col_name = \"colen_\" + \"_\".join(cols)\n",
    "                concat_expr = pl.col(cols[0]).cast(pl.Utf8)\n",
    "\n",
    "                for col_name in cols[1:]:\n",
    "                    concat_expr = concat_expr + \"_\" + pl.col(col_name).cast(pl.Utf8)\n",
    "\n",
    "                df = df.with_columns(concat_expr.alias(new_col_name).cast(pl.Categorical))\n",
    "\n",
    "            gc.collect()\n",
    "\n",
    "            mem_usage = sum(df.estimated_size() for col in df.columns) / (1024 * 1024)\n",
    "            print(f\"Memory usage: {mem_usage:.2f} MB\")\n",
    "            print(f\"Total number of columns: {len(df.columns)}\")\n",
    "\n",
    "        print(\"=\" * 20)\n",
    "\n",
    "    columns_to_encode = [\n",
    "        \"Host_Popularity_percentage\",\n",
    "        \"Guest_Popularity_percentage\",\n",
    "        \"Episode_Length_minutes\",\n",
    "        # \"Episode_Num\",\n",
    "        \"Podcast_Name\",\n",
    "        \"Publication_Day\",\n",
    "        # \"Publication_Time\",\n",
    "        \"Episode_Length_minutes_NaN\",\n",
    "        # \"Guest_Popularity_percentage_NaN\",\n",
    "        # \"HPperc_Int\",\n",
    "        # \"HPperc_Dec\",\n",
    "        \"ELen_Int\",\n",
    "        \"ELen_Dec\",\n",
    "        \"Length_per_Ads\",\n",
    "    ]\n",
    "\n",
    "    pair_size = [3]\n",
    "\n",
    "    # Pure polars implementation for combinations\n",
    "    for r in pair_size:\n",
    "        combinations_list = list(combinations(columns_to_encode, r))\n",
    "        batch_size = 20\n",
    "\n",
    "        print(\"\\n pair_size:\", r, \"\\n\")\n",
    "\n",
    "        for i in range(0, len(combinations_list), batch_size):\n",
    "            batch = combinations_list[i : i + batch_size]\n",
    "\n",
    "            for cols in tqdm(batch):\n",
    "                new_col_name = \"colen_\" + \"_\".join(cols)\n",
    "                concat_expr = pl.col(cols[0]).cast(pl.Utf8)\n",
    "\n",
    "                for col_name in cols[1:]:\n",
    "                    concat_expr = concat_expr + \"_\" + pl.col(col_name).cast(pl.Utf8)\n",
    "\n",
    "                df = df.with_columns(concat_expr.alias(new_col_name).cast(pl.Categorical))\n",
    "\n",
    "            gc.collect()\n",
    "\n",
    "            mem_usage = sum(df.estimated_size() for col in df.columns) / (1024 * 1024)\n",
    "            print(f\"Memory usage: {mem_usage:.2f} MB\")\n",
    "            print(f\"Total number of columns: {len(df.columns)}\")\n",
    "\n",
    "        print(\"=\" * 20)\n",
    "\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "36a5d7e1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T11:36:13.911029Z",
     "iopub.status.busy": "2025-04-21T11:36:13.910679Z",
     "iopub.status.idle": "2025-04-21T13:23:03.863592Z",
     "shell.execute_reply": "2025-04-21T13:23:03.862900Z"
    },
    "papermill": {
     "duration": 6409.95818,
     "end_time": "2025-04-21T13:23:03.864992",
     "exception": false,
     "start_time": "2025-04-21T11:36:13.906812",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmasaishi\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20250421_113621-la437m0o\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mswept-deluge-372\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/masaishi/playground-series-s5e4\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/masaishi/playground-series-s5e4/runs/la437m0o\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training fold 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_19/373779108.py:128: CategoricalRemappingWarning: Local categoricals have different encodings, expensive re-encoding is done to perform this merge operation. Consider using a StringCache or an Enum type if the categories are known in advance\n",
      "  df = df.with_columns(\n",
      "/tmp/ipykernel_19/373779108.py:186: CategoricalRemappingWarning: Local categoricals have different encodings, expensive re-encoding is done to perform this merge operation. Consider using a StringCache or an Enum type if the categories are known in advance\n",
      "  df = df.with_columns(pl.col(col).cast(pl.Utf8).cast(pl.Categorical))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " pair_size: 2 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]/tmp/ipykernel_19/373779108.py:232: CategoricalRemappingWarning: Local categoricals have different encodings, expensive re-encoding is done to perform this merge operation. Consider using a StringCache or an Enum type if the categories are known in advance\n",
      "  df = df.with_columns(concat_expr.alias(new_col_name).cast(pl.Categorical))\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:04<00:00,  4.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: 9212.45 MB\n",
      "Total number of columns: 53\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:04<00:00,  4.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: 19325.68 MB\n",
      "Total number of columns: 73\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:02<00:00,  9.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: 29711.36 MB\n",
      "Total number of columns: 93\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:02<00:00,  9.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: 42263.21 MB\n",
      "Total number of columns: 113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:01<00:00, 10.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: 56740.67 MB\n",
      "Total number of columns: 133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:02<00:00,  6.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: 75365.20 MB\n",
      "Total number of columns: 153\n",
      "====================\n",
      "\n",
      " pair_size: 3 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]/tmp/ipykernel_19/373779108.py:278: CategoricalRemappingWarning: Local categoricals have different encodings, expensive re-encoding is done to perform this merge operation. Consider using a StringCache or an Enum type if the categories are known in advance\n",
      "  df = df.with_columns(concat_expr.alias(new_col_name).cast(pl.Categorical))\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:12<00:00,  1.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: 121681.98 MB\n",
      "Total number of columns: 173\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:10<00:00,  1.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: 174838.32 MB\n",
      "Total number of columns: 193\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:07<00:00,  2.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: 224669.96 MB\n",
      "Total number of columns: 213\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:04<00:00,  4.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: 267638.26 MB\n",
      "Total number of columns: 233\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:00<00:00,  6.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: 275259.38 MB\n",
      "Total number of columns: 237\n",
      "====================\n",
      "\n",
      " pair_size: 2 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:00<00:00, 36.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: 2347.60 MB\n",
      "Total number of columns: 53\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:00<00:00, 37.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: 5092.93 MB\n",
      "Total number of columns: 73\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:00<00:00, 57.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: 7767.44 MB\n",
      "Total number of columns: 93\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:00<00:00, 53.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: 10959.61 MB\n",
      "Total number of columns: 113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:00<00:00, 50.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: 14658.88 MB\n",
      "Total number of columns: 133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:00<00:00, 39.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: 19632.85 MB\n",
      "Total number of columns: 153\n",
      "====================\n",
      "\n",
      " pair_size: 3 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:00<00:00, 21.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: 31234.64 MB\n",
      "Total number of columns: 173\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:00<00:00, 21.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: 44826.40 MB\n",
      "Total number of columns: 193\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:00<00:00, 24.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: 58283.84 MB\n",
      "Total number of columns: 213\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:00<00:00, 29.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: 70429.78 MB\n",
      "Total number of columns: 233\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:00<00:00, 31.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: 72740.43 MB\n",
      "Total number of columns: 237\n",
      "====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:1408: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\ttraining's rmse: 10.0533\tvalid_1's rmse: 12.133\n",
      "[200]\ttraining's rmse: 8.84844\tvalid_1's rmse: 12.1044\n",
      "[300]\ttraining's rmse: 8.02602\tvalid_1's rmse: 12.1019\n",
      "[400]\ttraining's rmse: 7.38103\tvalid_1's rmse: 12.0981\n",
      "[500]\ttraining's rmse: 6.83507\tvalid_1's rmse: 12.0994\n",
      "[600]\ttraining's rmse: 6.35682\tvalid_1's rmse: 12.1009\n",
      "Early stopping, best iteration is:\n",
      "[451]\ttraining's rmse: 7.09818\tvalid_1's rmse: 12.0973\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_19/373779108.py:128: CategoricalRemappingWarning: Local categoricals have different encodings, expensive re-encoding is done to perform this merge operation. Consider using a StringCache or an Enum type if the categories are known in advance\n",
      "  df = df.with_columns(\n",
      "/tmp/ipykernel_19/373779108.py:186: CategoricalRemappingWarning: Local categoricals have different encodings, expensive re-encoding is done to perform this merge operation. Consider using a StringCache or an Enum type if the categories are known in advance\n",
      "  df = df.with_columns(pl.col(col).cast(pl.Utf8).cast(pl.Categorical))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " pair_size: 2 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]/tmp/ipykernel_19/373779108.py:232: CategoricalRemappingWarning: Local categoricals have different encodings, expensive re-encoding is done to perform this merge operation. Consider using a StringCache or an Enum type if the categories are known in advance\n",
      "  df = df.with_columns(concat_expr.alias(new_col_name).cast(pl.Categorical))\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:00<00:00, 22.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: 3782.39 MB\n",
      "Total number of columns: 53\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:00<00:00, 23.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: 8120.29 MB\n",
      "Total number of columns: 73\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:00<00:00, 46.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: 12401.53 MB\n",
      "Total number of columns: 93\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:00<00:00, 48.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: 17545.73 MB\n",
      "Total number of columns: 113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:00<00:00, 44.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: 23491.24 MB\n",
      "Total number of columns: 133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:00<00:00, 32.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: 31324.50 MB\n",
      "Total number of columns: 153\n",
      "====================\n",
      "\n",
      " pair_size: 3 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]/tmp/ipykernel_19/373779108.py:278: CategoricalRemappingWarning: Local categoricals have different encodings, expensive re-encoding is done to perform this merge operation. Consider using a StringCache or an Enum type if the categories are known in advance\n",
      "  df = df.with_columns(concat_expr.alias(new_col_name).cast(pl.Categorical))\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:01<00:00, 11.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: 50194.94 MB\n",
      "Total number of columns: 173\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:01<00:00, 12.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: 72157.20 MB\n",
      "Total number of columns: 193\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:01<00:00, 14.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: 93503.60 MB\n",
      "Total number of columns: 213\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:00<00:00, 20.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: 112404.30 MB\n",
      "Total number of columns: 233\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:00<00:00, 22.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: 115857.57 MB\n",
      "Total number of columns: 237\n",
      "====================\n",
      "Fold 1 validation score: 12.097272935148485\n",
      "Training fold 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_19/373779108.py:128: CategoricalRemappingWarning: Local categoricals have different encodings, expensive re-encoding is done to perform this merge operation. Consider using a StringCache or an Enum type if the categories are known in advance\n",
      "  df = df.with_columns(\n",
      "/tmp/ipykernel_19/373779108.py:186: CategoricalRemappingWarning: Local categoricals have different encodings, expensive re-encoding is done to perform this merge operation. Consider using a StringCache or an Enum type if the categories are known in advance\n",
      "  df = df.with_columns(pl.col(col).cast(pl.Utf8).cast(pl.Categorical))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " pair_size: 2 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]/tmp/ipykernel_19/373779108.py:232: CategoricalRemappingWarning: Local categoricals have different encodings, expensive re-encoding is done to perform this merge operation. Consider using a StringCache or an Enum type if the categories are known in advance\n",
      "  df = df.with_columns(concat_expr.alias(new_col_name).cast(pl.Categorical))\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:04<00:00,  4.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: 9209.11 MB\n",
      "Total number of columns: 53\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:04<00:00,  4.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: 19316.35 MB\n",
      "Total number of columns: 73\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:02<00:00,  9.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: 29698.93 MB\n",
      "Total number of columns: 93\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:02<00:00,  9.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: 42247.73 MB\n",
      "Total number of columns: 113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:02<00:00,  9.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: 56722.23 MB\n",
      "Total number of columns: 133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:02<00:00,  7.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: 75344.20 MB\n",
      "Total number of columns: 153\n",
      "====================\n",
      "\n",
      " pair_size: 3 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]/tmp/ipykernel_19/373779108.py:278: CategoricalRemappingWarning: Local categoricals have different encodings, expensive re-encoding is done to perform this merge operation. Consider using a StringCache or an Enum type if the categories are known in advance\n",
      "  df = df.with_columns(concat_expr.alias(new_col_name).cast(pl.Categorical))\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:11<00:00,  1.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: 121503.75 MB\n",
      "Total number of columns: 173\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:10<00:00,  1.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: 174452.43 MB\n",
      "Total number of columns: 193\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:07<00:00,  2.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: 224126.62 MB\n",
      "Total number of columns: 213\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:04<00:00,  4.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: 267040.52 MB\n",
      "Total number of columns: 233\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:00<00:00,  6.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: 274652.06 MB\n",
      "Total number of columns: 237\n",
      "====================\n",
      "\n",
      " pair_size: 2 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:00<00:00, 37.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: 2345.22 MB\n",
      "Total number of columns: 53\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:00<00:00, 37.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: 5086.89 MB\n",
      "Total number of columns: 73\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:00<00:00, 58.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: 7760.12 MB\n",
      "Total number of columns: 93\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:00<00:00, 56.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: 10951.02 MB\n",
      "Total number of columns: 113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:00<00:00, 53.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: 14649.11 MB\n",
      "Total number of columns: 133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:00<00:00, 43.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: 19621.11 MB\n",
      "Total number of columns: 153\n",
      "====================\n",
      "\n",
      " pair_size: 3 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:00<00:00, 22.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: 31179.08 MB\n",
      "Total number of columns: 173\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:00<00:00, 21.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: 44692.93 MB\n",
      "Total number of columns: 193\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:00<00:00, 25.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: 58091.00 MB\n",
      "Total number of columns: 213\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:00<00:00, 29.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: 70219.25 MB\n",
      "Total number of columns: 233\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:00<00:00, 30.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: 72526.44 MB\n",
      "Total number of columns: 237\n",
      "====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:1408: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 0 that is less than the current step 651. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 0 that is less than the current step 651. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 0 that is less than the current step 651. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 50 that is less than the current step 651. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 50 that is less than the current step 651. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 50 that is less than the current step 651. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\ttraining's rmse: 10.055\tvalid_1's rmse: 12.1507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 100 that is less than the current step 651. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 100 that is less than the current step 651. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 100 that is less than the current step 651. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 150 that is less than the current step 651. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 150 that is less than the current step 651. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 150 that is less than the current step 651. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[200]\ttraining's rmse: 8.84871\tvalid_1's rmse: 12.1254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 200 that is less than the current step 651. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 200 that is less than the current step 651. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 200 that is less than the current step 651. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 250 that is less than the current step 651. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 250 that is less than the current step 651. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 250 that is less than the current step 651. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[300]\ttraining's rmse: 8.05825\tvalid_1's rmse: 12.1205\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 300 that is less than the current step 651. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 300 that is less than the current step 651. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 300 that is less than the current step 651. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 350 that is less than the current step 651. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 350 that is less than the current step 651. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 350 that is less than the current step 651. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[400]\ttraining's rmse: 7.40458\tvalid_1's rmse: 12.1155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 400 that is less than the current step 651. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 400 that is less than the current step 651. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 400 that is less than the current step 651. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 450 that is less than the current step 651. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 450 that is less than the current step 651. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 450 that is less than the current step 651. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[500]\ttraining's rmse: 6.83358\tvalid_1's rmse: 12.1165\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 500 that is less than the current step 651. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 500 that is less than the current step 651. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 500 that is less than the current step 651. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 550 that is less than the current step 651. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 550 that is less than the current step 651. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 550 that is less than the current step 651. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[600]\ttraining's rmse: 6.35983\tvalid_1's rmse: 12.1187\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 600 that is less than the current step 651. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 600 that is less than the current step 651. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 600 that is less than the current step 651. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 650 that is less than the current step 651. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 650 that is less than the current step 651. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 650 that is less than the current step 651. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[461]\ttraining's rmse: 7.05815\tvalid_1's rmse: 12.1149\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_19/373779108.py:128: CategoricalRemappingWarning: Local categoricals have different encodings, expensive re-encoding is done to perform this merge operation. Consider using a StringCache or an Enum type if the categories are known in advance\n",
      "  df = df.with_columns(\n",
      "/tmp/ipykernel_19/373779108.py:186: CategoricalRemappingWarning: Local categoricals have different encodings, expensive re-encoding is done to perform this merge operation. Consider using a StringCache or an Enum type if the categories are known in advance\n",
      "  df = df.with_columns(pl.col(col).cast(pl.Utf8).cast(pl.Categorical))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " pair_size: 2 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]/tmp/ipykernel_19/373779108.py:232: CategoricalRemappingWarning: Local categoricals have different encodings, expensive re-encoding is done to perform this merge operation. Consider using a StringCache or an Enum type if the categories are known in advance\n",
      "  df = df.with_columns(concat_expr.alias(new_col_name).cast(pl.Categorical))\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:00<00:00, 20.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: 3779.62 MB\n",
      "Total number of columns: 53\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:01<00:00, 19.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: 8112.61 MB\n",
      "Total number of columns: 73\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:00<00:00, 49.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: 12391.73 MB\n",
      "Total number of columns: 93\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:00<00:00, 47.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: 17533.81 MB\n",
      "Total number of columns: 113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:00<00:00, 42.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: 23477.23 MB\n",
      "Total number of columns: 133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:00<00:00, 31.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: 31308.24 MB\n",
      "Total number of columns: 153\n",
      "====================\n",
      "\n",
      " pair_size: 3 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]/tmp/ipykernel_19/373779108.py:278: CategoricalRemappingWarning: Local categoricals have different encodings, expensive re-encoding is done to perform this merge operation. Consider using a StringCache or an Enum type if the categories are known in advance\n",
      "  df = df.with_columns(concat_expr.alias(new_col_name).cast(pl.Categorical))\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:01<00:00, 12.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: 50108.12 MB\n",
      "Total number of columns: 173\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:01<00:00, 12.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: 71955.84 MB\n",
      "Total number of columns: 193\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:01<00:00, 15.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: 93214.20 MB\n",
      "Total number of columns: 213\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:01<00:00, 19.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: 112087.20 MB\n",
      "Total number of columns: 233\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:00<00:00, 19.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: 115535.02 MB\n",
      "Total number of columns: 237\n",
      "====================\n",
      "Fold 2 validation score: 12.11493535623026\n",
      "Training fold 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_19/373779108.py:128: CategoricalRemappingWarning: Local categoricals have different encodings, expensive re-encoding is done to perform this merge operation. Consider using a StringCache or an Enum type if the categories are known in advance\n",
      "  df = df.with_columns(\n",
      "/tmp/ipykernel_19/373779108.py:186: CategoricalRemappingWarning: Local categoricals have different encodings, expensive re-encoding is done to perform this merge operation. Consider using a StringCache or an Enum type if the categories are known in advance\n",
      "  df = df.with_columns(pl.col(col).cast(pl.Utf8).cast(pl.Categorical))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " pair_size: 2 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]/tmp/ipykernel_19/373779108.py:232: CategoricalRemappingWarning: Local categoricals have different encodings, expensive re-encoding is done to perform this merge operation. Consider using a StringCache or an Enum type if the categories are known in advance\n",
      "  df = df.with_columns(concat_expr.alias(new_col_name).cast(pl.Categorical))\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:04<00:00,  4.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: 9212.20 MB\n",
      "Total number of columns: 53\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:04<00:00,  4.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: 19325.02 MB\n",
      "Total number of columns: 73\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:01<00:00, 10.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: 29710.24 MB\n",
      "Total number of columns: 93\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:02<00:00,  9.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: 42261.41 MB\n",
      "Total number of columns: 113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:02<00:00,  9.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: 56738.24 MB\n",
      "Total number of columns: 133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:02<00:00,  6.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: 75361.69 MB\n",
      "Total number of columns: 153\n",
      "====================\n",
      "\n",
      " pair_size: 3 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]/tmp/ipykernel_19/373779108.py:278: CategoricalRemappingWarning: Local categoricals have different encodings, expensive re-encoding is done to perform this merge operation. Consider using a StringCache or an Enum type if the categories are known in advance\n",
      "  df = df.with_columns(concat_expr.alias(new_col_name).cast(pl.Categorical))\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:11<00:00,  1.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: 121674.49 MB\n",
      "Total number of columns: 173\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:10<00:00,  1.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: 174821.30 MB\n",
      "Total number of columns: 193\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:07<00:00,  2.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: 224639.77 MB\n",
      "Total number of columns: 213\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:04<00:00,  4.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: 267599.78 MB\n",
      "Total number of columns: 233\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:00<00:00,  6.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: 275220.36 MB\n",
      "Total number of columns: 237\n",
      "====================\n",
      "\n",
      " pair_size: 2 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:00<00:00, 37.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: 2347.96 MB\n",
      "Total number of columns: 53\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:00<00:00, 37.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: 5093.88 MB\n",
      "Total number of columns: 73\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:00<00:00, 57.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: 7768.87 MB\n",
      "Total number of columns: 93\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:00<00:00, 55.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: 10961.33 MB\n",
      "Total number of columns: 113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:00<00:00, 50.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: 14661.03 MB\n",
      "Total number of columns: 133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:00<00:00, 42.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: 19636.10 MB\n",
      "Total number of columns: 153\n",
      "====================\n",
      "\n",
      " pair_size: 3 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:00<00:00, 22.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: 31239.11 MB\n",
      "Total number of columns: 173\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:00<00:00, 21.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: 44834.91 MB\n",
      "Total number of columns: 193\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:00<00:00, 25.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: 58295.19 MB\n",
      "Total number of columns: 213\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:00<00:00, 30.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: 70441.16 MB\n",
      "Total number of columns: 233\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:00<00:00, 31.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: 72752.64 MB\n",
      "Total number of columns: 237\n",
      "====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:1408: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 0 that is less than the current step 652. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 0 that is less than the current step 652. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 0 that is less than the current step 652. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 50 that is less than the current step 652. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 50 that is less than the current step 652. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 50 that is less than the current step 652. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\ttraining's rmse: 10.0618\tvalid_1's rmse: 12.1759\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 100 that is less than the current step 652. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 100 that is less than the current step 652. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 100 that is less than the current step 652. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 150 that is less than the current step 652. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 150 that is less than the current step 652. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 150 that is less than the current step 652. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[200]\ttraining's rmse: 8.84667\tvalid_1's rmse: 12.1411\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 200 that is less than the current step 652. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 200 that is less than the current step 652. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 200 that is less than the current step 652. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 250 that is less than the current step 652. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 250 that is less than the current step 652. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 250 that is less than the current step 652. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[300]\ttraining's rmse: 8.0237\tvalid_1's rmse: 12.1373\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 300 that is less than the current step 652. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 300 that is less than the current step 652. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 300 that is less than the current step 652. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 350 that is less than the current step 652. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 350 that is less than the current step 652. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 350 that is less than the current step 652. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[400]\ttraining's rmse: 7.36984\tvalid_1's rmse: 12.1342\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 400 that is less than the current step 652. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 400 that is less than the current step 652. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 400 that is less than the current step 652. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 450 that is less than the current step 652. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 450 that is less than the current step 652. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 450 that is less than the current step 652. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[500]\ttraining's rmse: 6.82381\tvalid_1's rmse: 12.1363\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 500 that is less than the current step 652. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 500 that is less than the current step 652. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 500 that is less than the current step 652. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 550 that is less than the current step 652. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 550 that is less than the current step 652. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 550 that is less than the current step 652. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[600]\ttraining's rmse: 6.35265\tvalid_1's rmse: 12.1381\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 600 that is less than the current step 652. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 600 that is less than the current step 652. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 600 that is less than the current step 652. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[422]\ttraining's rmse: 7.24242\tvalid_1's rmse: 12.1334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_19/373779108.py:128: CategoricalRemappingWarning: Local categoricals have different encodings, expensive re-encoding is done to perform this merge operation. Consider using a StringCache or an Enum type if the categories are known in advance\n",
      "  df = df.with_columns(\n",
      "/tmp/ipykernel_19/373779108.py:186: CategoricalRemappingWarning: Local categoricals have different encodings, expensive re-encoding is done to perform this merge operation. Consider using a StringCache or an Enum type if the categories are known in advance\n",
      "  df = df.with_columns(pl.col(col).cast(pl.Utf8).cast(pl.Categorical))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " pair_size: 2 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]/tmp/ipykernel_19/373779108.py:232: CategoricalRemappingWarning: Local categoricals have different encodings, expensive re-encoding is done to perform this merge operation. Consider using a StringCache or an Enum type if the categories are known in advance\n",
      "  df = df.with_columns(concat_expr.alias(new_col_name).cast(pl.Categorical))\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:00<00:00, 21.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: 3782.33 MB\n",
      "Total number of columns: 53\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:00<00:00, 24.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: 8120.15 MB\n",
      "Total number of columns: 73\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:00<00:00, 47.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: 12401.35 MB\n",
      "Total number of columns: 93\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:00<00:00, 46.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: 17545.49 MB\n",
      "Total number of columns: 113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:00<00:00, 44.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: 23490.95 MB\n",
      "Total number of columns: 133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:00<00:00, 31.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: 31324.09 MB\n",
      "Total number of columns: 153\n",
      "====================\n",
      "\n",
      " pair_size: 3 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]/tmp/ipykernel_19/373779108.py:278: CategoricalRemappingWarning: Local categoricals have different encodings, expensive re-encoding is done to perform this merge operation. Consider using a StringCache or an Enum type if the categories are known in advance\n",
      "  df = df.with_columns(concat_expr.alias(new_col_name).cast(pl.Categorical))\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:01<00:00, 12.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: 50194.38 MB\n",
      "Total number of columns: 173\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:01<00:00, 12.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: 72156.34 MB\n",
      "Total number of columns: 193\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:01<00:00, 15.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: 93502.58 MB\n",
      "Total number of columns: 213\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:01<00:00, 19.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: 112403.01 MB\n",
      "Total number of columns: 233\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:00<00:00, 23.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: 115856.27 MB\n",
      "Total number of columns: 237\n",
      "====================\n",
      "Fold 3 validation score: 12.133404259916572\n",
      "Training fold 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_19/373779108.py:128: CategoricalRemappingWarning: Local categoricals have different encodings, expensive re-encoding is done to perform this merge operation. Consider using a StringCache or an Enum type if the categories are known in advance\n",
      "  df = df.with_columns(\n",
      "/tmp/ipykernel_19/373779108.py:186: CategoricalRemappingWarning: Local categoricals have different encodings, expensive re-encoding is done to perform this merge operation. Consider using a StringCache or an Enum type if the categories are known in advance\n",
      "  df = df.with_columns(pl.col(col).cast(pl.Utf8).cast(pl.Categorical))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " pair_size: 2 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]/tmp/ipykernel_19/373779108.py:232: CategoricalRemappingWarning: Local categoricals have different encodings, expensive re-encoding is done to perform this merge operation. Consider using a StringCache or an Enum type if the categories are known in advance\n",
      "  df = df.with_columns(concat_expr.alias(new_col_name).cast(pl.Categorical))\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:04<00:00,  4.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: 9212.91 MB\n",
      "Total number of columns: 53\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:04<00:00,  4.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: 19331.23 MB\n",
      "Total number of columns: 73\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:01<00:00, 10.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: 29718.47 MB\n",
      "Total number of columns: 93\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:02<00:00,  9.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: 42272.05 MB\n",
      "Total number of columns: 113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:01<00:00, 10.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: 56751.24 MB\n",
      "Total number of columns: 133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:02<00:00,  7.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: 75379.76 MB\n",
      "Total number of columns: 153\n",
      "====================\n",
      "\n",
      " pair_size: 3 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]/tmp/ipykernel_19/373779108.py:278: CategoricalRemappingWarning: Local categoricals have different encodings, expensive re-encoding is done to perform this merge operation. Consider using a StringCache or an Enum type if the categories are known in advance\n",
      "  df = df.with_columns(concat_expr.alias(new_col_name).cast(pl.Categorical))\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:11<00:00,  1.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: 121761.56 MB\n",
      "Total number of columns: 173\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:10<00:00,  1.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: 175000.81 MB\n",
      "Total number of columns: 193\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:07<00:00,  2.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: 224882.80 MB\n",
      "Total number of columns: 213\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:04<00:00,  4.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: 267873.70 MB\n",
      "Total number of columns: 233\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:00<00:00,  6.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: 275499.33 MB\n",
      "Total number of columns: 237\n",
      "====================\n",
      "\n",
      " pair_size: 2 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:00<00:00, 36.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: 2349.82 MB\n",
      "Total number of columns: 53\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:00<00:00, 38.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: 5096.10 MB\n",
      "Total number of columns: 73\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:00<00:00, 56.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: 7771.71 MB\n",
      "Total number of columns: 93\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:00<00:00, 56.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: 10964.73 MB\n",
      "Total number of columns: 113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:00<00:00, 52.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: 14664.72 MB\n",
      "Total number of columns: 133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:00<00:00, 41.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: 19640.20 MB\n",
      "Total number of columns: 153\n",
      "====================\n",
      "\n",
      " pair_size: 3 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:00<00:00, 21.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: 31259.94 MB\n",
      "Total number of columns: 173\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:00<00:00, 22.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: 44868.95 MB\n",
      "Total number of columns: 193\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:00<00:00, 25.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: 58331.20 MB\n",
      "Total number of columns: 213\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:00<00:00, 30.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: 70480.77 MB\n",
      "Total number of columns: 233\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:00<00:00, 30.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: 72791.50 MB\n",
      "Total number of columns: 237\n",
      "====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:1408: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 0 that is less than the current step 653. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 0 that is less than the current step 653. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 0 that is less than the current step 653. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 50 that is less than the current step 653. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 50 that is less than the current step 653. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 50 that is less than the current step 653. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\ttraining's rmse: 10.0736\tvalid_1's rmse: 12.1233\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 100 that is less than the current step 653. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 100 that is less than the current step 653. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 100 that is less than the current step 653. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 150 that is less than the current step 653. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 150 that is less than the current step 653. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 150 that is less than the current step 653. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[200]\ttraining's rmse: 8.86089\tvalid_1's rmse: 12.0914\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 200 that is less than the current step 653. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 200 that is less than the current step 653. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 200 that is less than the current step 653. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 250 that is less than the current step 653. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 250 that is less than the current step 653. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 250 that is less than the current step 653. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[300]\ttraining's rmse: 8.05607\tvalid_1's rmse: 12.0859\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 300 that is less than the current step 653. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 300 that is less than the current step 653. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 300 that is less than the current step 653. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 350 that is less than the current step 653. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 350 that is less than the current step 653. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 350 that is less than the current step 653. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[400]\ttraining's rmse: 7.40899\tvalid_1's rmse: 12.085\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 400 that is less than the current step 653. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 400 that is less than the current step 653. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 400 that is less than the current step 653. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 450 that is less than the current step 653. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 450 that is less than the current step 653. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 450 that is less than the current step 653. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[500]\ttraining's rmse: 6.86862\tvalid_1's rmse: 12.0863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 500 that is less than the current step 653. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 500 that is less than the current step 653. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 500 that is less than the current step 653. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[352]\ttraining's rmse: 7.70397\tvalid_1's rmse: 12.0835\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_19/373779108.py:128: CategoricalRemappingWarning: Local categoricals have different encodings, expensive re-encoding is done to perform this merge operation. Consider using a StringCache or an Enum type if the categories are known in advance\n",
      "  df = df.with_columns(\n",
      "/tmp/ipykernel_19/373779108.py:186: CategoricalRemappingWarning: Local categoricals have different encodings, expensive re-encoding is done to perform this merge operation. Consider using a StringCache or an Enum type if the categories are known in advance\n",
      "  df = df.with_columns(pl.col(col).cast(pl.Utf8).cast(pl.Categorical))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " pair_size: 2 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]/tmp/ipykernel_19/373779108.py:232: CategoricalRemappingWarning: Local categoricals have different encodings, expensive re-encoding is done to perform this merge operation. Consider using a StringCache or an Enum type if the categories are known in advance\n",
      "  df = df.with_columns(concat_expr.alias(new_col_name).cast(pl.Categorical))\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:00<00:00, 21.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: 3784.66 MB\n",
      "Total number of columns: 53\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:00<00:00, 23.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: 8125.24 MB\n",
      "Total number of columns: 73\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:00<00:00, 48.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: 12408.01 MB\n",
      "Total number of columns: 93\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:00<00:00, 47.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: 17553.73 MB\n",
      "Total number of columns: 113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:00<00:00, 43.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: 23500.73 MB\n",
      "Total number of columns: 133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:00<00:00, 25.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: 31336.42 MB\n",
      "Total number of columns: 153\n",
      "====================\n",
      "\n",
      " pair_size: 3 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]/tmp/ipykernel_19/373779108.py:278: CategoricalRemappingWarning: Local categoricals have different encodings, expensive re-encoding is done to perform this merge operation. Consider using a StringCache or an Enum type if the categories are known in advance\n",
      "  df = df.with_columns(concat_expr.alias(new_col_name).cast(pl.Categorical))\n",
      " 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 15/20 [00:01<00:00, 11.87it/s]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 550 that is less than the current step 653. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 550 that is less than the current step 653. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 550 that is less than the current step 653. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:01<00:00, 11.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: 50237.09 MB\n",
      "Total number of columns: 173\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:01<00:00, 12.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: 72236.56 MB\n",
      "Total number of columns: 193\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:01<00:00, 15.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: 93603.32 MB\n",
      "Total number of columns: 213\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:00<00:00, 20.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: 112515.30 MB\n",
      "Total number of columns: 233\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:00<00:00, 21.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: 115970.51 MB\n",
      "Total number of columns: 237\n",
      "====================\n",
      "Fold 4 validation score: 12.083512857871021\n",
      "Training fold 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_19/373779108.py:128: CategoricalRemappingWarning: Local categoricals have different encodings, expensive re-encoding is done to perform this merge operation. Consider using a StringCache or an Enum type if the categories are known in advance\n",
      "  df = df.with_columns(\n",
      "/tmp/ipykernel_19/373779108.py:186: CategoricalRemappingWarning: Local categoricals have different encodings, expensive re-encoding is done to perform this merge operation. Consider using a StringCache or an Enum type if the categories are known in advance\n",
      "  df = df.with_columns(pl.col(col).cast(pl.Utf8).cast(pl.Categorical))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " pair_size: 2 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]/tmp/ipykernel_19/373779108.py:232: CategoricalRemappingWarning: Local categoricals have different encodings, expensive re-encoding is done to perform this merge operation. Consider using a StringCache or an Enum type if the categories are known in advance\n",
      "  df = df.with_columns(concat_expr.alias(new_col_name).cast(pl.Categorical))\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:04<00:00,  4.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: 9212.34 MB\n",
      "Total number of columns: 53\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:04<00:00,  4.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: 19325.70 MB\n",
      "Total number of columns: 73\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:01<00:00, 10.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: 29711.15 MB\n",
      "Total number of columns: 93\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:02<00:00,  9.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: 42263.15 MB\n",
      "Total number of columns: 113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:01<00:00, 10.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: 56740.72 MB\n",
      "Total number of columns: 133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:02<00:00,  6.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: 75363.98 MB\n",
      "Total number of columns: 153\n",
      "====================\n",
      "\n",
      " pair_size: 3 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]/tmp/ipykernel_19/373779108.py:278: CategoricalRemappingWarning: Local categoricals have different encodings, expensive re-encoding is done to perform this merge operation. Consider using a StringCache or an Enum type if the categories are known in advance\n",
      "  df = df.with_columns(concat_expr.alias(new_col_name).cast(pl.Categorical))\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:11<00:00,  1.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: 121675.35 MB\n",
      "Total number of columns: 173\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:10<00:00,  1.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: 174826.22 MB\n",
      "Total number of columns: 193\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:07<00:00,  2.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: 224653.11 MB\n",
      "Total number of columns: 213\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:04<00:00,  4.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: 267619.58 MB\n",
      "Total number of columns: 233\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:00<00:00,  6.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: 275240.54 MB\n",
      "Total number of columns: 237\n",
      "====================\n",
      "\n",
      " pair_size: 2 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:00<00:00, 37.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: 2347.37 MB\n",
      "Total number of columns: 53\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:00<00:00, 38.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: 5092.56 MB\n",
      "Total number of columns: 73\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:00<00:00, 57.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: 7767.22 MB\n",
      "Total number of columns: 93\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:00<00:00, 55.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: 10959.54 MB\n",
      "Total number of columns: 113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:00<00:00, 53.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: 14658.61 MB\n",
      "Total number of columns: 133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:00<00:00, 42.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: 19632.80 MB\n",
      "Total number of columns: 153\n",
      "====================\n",
      "\n",
      " pair_size: 3 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:00<00:00, 21.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: 31236.98 MB\n",
      "Total number of columns: 173\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:00<00:00, 21.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: 44829.57 MB\n",
      "Total number of columns: 193\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:00<00:00, 25.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: 58289.12 MB\n",
      "Total number of columns: 213\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:00<00:00, 30.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: 70439.40 MB\n",
      "Total number of columns: 233\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:00<00:00, 30.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: 72750.23 MB\n",
      "Total number of columns: 237\n",
      "====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:1408: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 0 that is less than the current step 654. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 0 that is less than the current step 654. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 0 that is less than the current step 654. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 50 that is less than the current step 654. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 50 that is less than the current step 654. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 50 that is less than the current step 654. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\ttraining's rmse: 10.0675\tvalid_1's rmse: 12.1041\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 100 that is less than the current step 654. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 100 that is less than the current step 654. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 100 that is less than the current step 654. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 150 that is less than the current step 654. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 150 that is less than the current step 654. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 150 that is less than the current step 654. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[200]\ttraining's rmse: 8.85397\tvalid_1's rmse: 12.0678\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 200 that is less than the current step 654. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 200 that is less than the current step 654. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 200 that is less than the current step 654. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 250 that is less than the current step 654. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 250 that is less than the current step 654. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 250 that is less than the current step 654. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[300]\ttraining's rmse: 8.03798\tvalid_1's rmse: 12.0672\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 300 that is less than the current step 654. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 300 that is less than the current step 654. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 300 that is less than the current step 654. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 350 that is less than the current step 654. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 350 that is less than the current step 654. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 350 that is less than the current step 654. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[400]\ttraining's rmse: 7.40728\tvalid_1's rmse: 12.0634\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 400 that is less than the current step 654. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 400 that is less than the current step 654. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 400 that is less than the current step 654. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 450 that is less than the current step 654. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 450 that is less than the current step 654. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 450 that is less than the current step 654. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[500]\ttraining's rmse: 6.85458\tvalid_1's rmse: 12.0634\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 500 that is less than the current step 654. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 500 that is less than the current step 654. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 500 that is less than the current step 654. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 550 that is less than the current step 654. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 550 that is less than the current step 654. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 550 that is less than the current step 654. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[600]\ttraining's rmse: 6.37508\tvalid_1's rmse: 12.0666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 600 that is less than the current step 654. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 600 that is less than the current step 654. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 600 that is less than the current step 654. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[411]\ttraining's rmse: 7.33769\tvalid_1's rmse: 12.0622\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_19/373779108.py:128: CategoricalRemappingWarning: Local categoricals have different encodings, expensive re-encoding is done to perform this merge operation. Consider using a StringCache or an Enum type if the categories are known in advance\n",
      "  df = df.with_columns(\n",
      "/tmp/ipykernel_19/373779108.py:186: CategoricalRemappingWarning: Local categoricals have different encodings, expensive re-encoding is done to perform this merge operation. Consider using a StringCache or an Enum type if the categories are known in advance\n",
      "  df = df.with_columns(pl.col(col).cast(pl.Utf8).cast(pl.Categorical))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " pair_size: 2 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]/tmp/ipykernel_19/373779108.py:232: CategoricalRemappingWarning: Local categoricals have different encodings, expensive re-encoding is done to perform this merge operation. Consider using a StringCache or an Enum type if the categories are known in advance\n",
      "  df = df.with_columns(concat_expr.alias(new_col_name).cast(pl.Categorical))\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:00<00:00, 21.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: 3782.33 MB\n",
      "Total number of columns: 53\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:00<00:00, 23.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: 8120.15 MB\n",
      "Total number of columns: 73\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:00<00:00, 49.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: 12401.35 MB\n",
      "Total number of columns: 93\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:00<00:00, 47.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: 17545.49 MB\n",
      "Total number of columns: 113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:00<00:00, 44.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: 23490.95 MB\n",
      "Total number of columns: 133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:00<00:00, 31.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: 31324.09 MB\n",
      "Total number of columns: 153\n",
      "====================\n",
      "\n",
      " pair_size: 3 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]/tmp/ipykernel_19/373779108.py:278: CategoricalRemappingWarning: Local categoricals have different encodings, expensive re-encoding is done to perform this merge operation. Consider using a StringCache or an Enum type if the categories are known in advance\n",
      "  df = df.with_columns(concat_expr.alias(new_col_name).cast(pl.Categorical))\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:01<00:00, 12.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: 50194.38 MB\n",
      "Total number of columns: 173\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:01<00:00, 12.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: 72156.34 MB\n",
      "Total number of columns: 193\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:01<00:00, 15.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: 93502.58 MB\n",
      "Total number of columns: 213\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:01<00:00, 19.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: 112403.01 MB\n",
      "Total number of columns: 233\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:00<00:00, 19.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: 115856.27 MB\n",
      "Total number of columns: 237\n",
      "====================\n",
      "Fold 5 validation score: 12.062151672535624\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: fold_1_val_score ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: fold_2_val_score ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: fold_3_val_score ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: fold_4_val_score ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: fold_5_val_score ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    training/rmse ‚ñà‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     valid_1/rmse ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: fold_1_val_score 12.09727\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: fold_2_val_score 12.11494\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: fold_3_val_score 12.1334\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: fold_4_val_score 12.08351\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: fold_5_val_score 12.06215\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    training/rmse 6.13995\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     valid_1/rmse 12.10138\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run \u001b[33mswept-deluge-372\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/masaishi/playground-series-s5e4/runs/la437m0o\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at: \u001b[34m\u001b[4mhttps://wandb.ai/masaishi/playground-series-s5e4\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 132 media file(s), 264 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250421_113621-la437m0o/logs\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import TargetEncoder\n",
    "import lightgbm as lgb\n",
    "import wandb\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "from dataclasses import asdict\n",
    "\n",
    "\n",
    "# Custom wandb callback\n",
    "class WandbCallback:\n",
    "    def __init__(self, log_every=50, log_feature_importance=True):\n",
    "        self.log_every = log_every\n",
    "        self.iteration = 0\n",
    "        self.log_feature_importance = log_feature_importance\n",
    "\n",
    "    def __call__(self, env):\n",
    "        if self.iteration % self.log_every == 0:\n",
    "            metrics = {}\n",
    "            for dataset_name, eval_name, value, _ in env.evaluation_result_list:\n",
    "                metric_name = f\"{dataset_name}/{eval_name}\"\n",
    "                metrics[metric_name] = value\n",
    "\n",
    "            # Log feature importance if requested\n",
    "            if self.log_feature_importance and hasattr(env, \"model\"):\n",
    "                feature_names = env.model.feature_name()\n",
    "                importance = env.model.feature_importance(importance_type=\"split\")\n",
    "                feature_importance = {name: imp for name, imp in zip(feature_names, importance)}\n",
    "\n",
    "                # Log to wandb\n",
    "                wandb.log(\n",
    "                    {\n",
    "                        \"feature_importance\": wandb.Table(\n",
    "                            data=[[k, v] for k, v in sorted(feature_importance.items(), key=lambda x: x[1], reverse=True)],\n",
    "                            columns=[\"feature\", \"importance\"],\n",
    "                        )\n",
    "                    },\n",
    "                    step=self.iteration,\n",
    "                )\n",
    "\n",
    "                # Log top 10 features\n",
    "                wandb.log(\n",
    "                    {\n",
    "                        \"feature_importance_plot\": wandb.plot.bar(\n",
    "                            wandb.Table(\n",
    "                                data=[[k, v] for k, v in sorted(feature_importance.items(), key=lambda x: x[1], reverse=True)[:10]],\n",
    "                                columns=[\"feature\", \"importance\"],\n",
    "                            ),\n",
    "                            \"feature\",\n",
    "                            \"importance\",\n",
    "                            title=\"Top Feature Importance\",\n",
    "                        )\n",
    "                    },\n",
    "                    step=self.iteration,\n",
    "                )\n",
    "\n",
    "            wandb.log(metrics, step=self.iteration)\n",
    "\n",
    "        self.iteration += 1\n",
    "        return False\n",
    "        \n",
    "def get_dfs_with_kfold(cfg):\n",
    "    # Read CSV files\n",
    "    df_train = pl.read_csv(cfg.train_path)\n",
    "    df_test = pl.read_csv(cfg.test_path)\n",
    "    \n",
    "    test_ids = df_test[\"id\"]\n",
    "\n",
    "    df_train = df_train.drop(\"id\")\n",
    "    df_test = df_test.drop(\"id\")\n",
    "    \n",
    "    target_col = \"Listening_Time_minutes\"\n",
    "    \n",
    "    # Load and prepare podcast data\n",
    "    df_pltpd = pl.read_csv(cfg.pltpd_path)\n",
    "    df_pltpd = df_pltpd.filter(pl.col(\"Listening_Time_minutes\").is_not_null())\n",
    "    y_pltpd = df_pltpd[\"Listening_Time_minutes\"]\n",
    "    df_pltpd = df_pltpd.drop(\"Listening_Time_minutes\")\n",
    "    df_pltpd = df_pltpd.with_columns(pl.col(\"Number_of_Ads\").cast(pl.Float64))\n",
    "    \n",
    "    # Create KFold\n",
    "    kf = KFold(n_splits=cfg.num_fold, random_state=42, shuffle=True)\n",
    "    \n",
    "    # Get all row indices\n",
    "    all_indices = np.arange(len(df_train))\n",
    "    \n",
    "    # Initialize predictions array for test data\n",
    "    test_predictions = np.zeros(len(df_test))\n",
    "    \n",
    "    # Train models with cross-validation\n",
    "    for fold, (idx_train, idx_valid) in enumerate(kf.split(all_indices)):\n",
    "        print(f\"Training fold {fold+1}/{cfg.num_fold}\")\n",
    "\n",
    "        # Use direct indexing to select rows from the Polars DataFrame\n",
    "        X_train = df_train.drop(target_col)[idx_train.tolist()]\n",
    "        y_train = df_train.select(target_col)[idx_train.tolist()]\n",
    "        X_valid = df_train.drop(target_col)[idx_valid.tolist()]\n",
    "        y_valid = df_train.select(target_col)[idx_valid.tolist()]\n",
    "        \n",
    "        # Concatenate with podcast data\n",
    "        X_train = pl.concat([X_train, df_pltpd], how=\"vertical\")\n",
    "        y_train = pl.concat([y_train, pl.DataFrame({target_col: y_pltpd})], how=\"vertical\")\n",
    "\n",
    "        # Only use this line for debugging with a small sample\n",
    "        if hasattr(cfg, 'debug') and cfg.debug:\n",
    "            X_train = X_train.sample(100)\n",
    "            y_train = y_train.sample(100)\n",
    "\n",
    "        # Preprocess\n",
    "        X_train = preprocess(X_train)\n",
    "        X_valid = preprocess(X_valid, X_train)\n",
    "        \n",
    "        # Feature engineering\n",
    "        df_train_with_target = X_train.with_columns(y_train.select(pl.col(target_col)).rename({target_col: target_col}))\n",
    "        X_train = feature_eng(X_train, df_train_with_target)\n",
    "        X_valid = feature_eng(X_valid, df_train_with_target)\n",
    "        \n",
    "        # Encoding\n",
    "        before_encode_len = len(X_train.columns)\n",
    "        X_train = cols_encode(X_train)\n",
    "        X_valid = cols_encode(X_valid)\n",
    "        \n",
    "        # Target encoding - we need to apply this with Polars\n",
    "        encoded_columns = X_train.columns[before_encode_len:]\n",
    "        \n",
    "        # For target encoding, we'll use a custom function that works with polars\n",
    "        X_train, X_valid, encoder = target_encode_polars(\n",
    "            X_train, y_train, X_valid, \n",
    "            columns=encoded_columns,\n",
    "            random_state=cfg.random_state\n",
    "        )\n",
    "\n",
    "        X_train = cast_numeric_dtypes(X_train)\n",
    "        X_valid = cast_numeric_dtypes(X_valid)\n",
    "        \n",
    "        # Convert to numpy arrays for LightGBM\n",
    "        X_train_arr = X_train.to_pandas()\n",
    "        y_train_arr = y_train.to_pandas()\n",
    "        X_valid_arr = X_valid.to_pandas()\n",
    "        y_valid_arr = y_valid.to_pandas()\n",
    "        \n",
    "        # Train model\n",
    "        model = lgb.LGBMRegressor(\n",
    "            n_iter=cfg.n_iter,\n",
    "            max_depth=cfg.max_depth,\n",
    "            num_leaves=cfg.num_leaves,\n",
    "            colsample_bytree=cfg.colsample_bytree,\n",
    "            learning_rate=cfg.learning_rate,\n",
    "            objective=cfg.objective,\n",
    "            metric=cfg.metric,\n",
    "            verbosity=cfg.verbosity,\n",
    "            random_state=42,\n",
    "        )\n",
    "        \n",
    "        model.fit(\n",
    "            X_train_arr, y_train_arr,\n",
    "            eval_set=[(X_train_arr, y_train_arr), (X_valid_arr, y_valid_arr)],\n",
    "            callbacks=[\n",
    "                lgb.log_evaluation(cfg.log_eval),\n",
    "                lgb.early_stopping(cfg.early_stopping),\n",
    "                WandbCallback(log_every=50),\n",
    "            ],\n",
    "            feature_name=X_train.columns,\n",
    "        )\n",
    "        \n",
    "        # Process test data\n",
    "        X_test = preprocess(df_test, X_train)\n",
    "        X_test = feature_eng(X_test, df_train_with_target)\n",
    "        X_test = cols_encode(X_test)\n",
    "        X_test = apply_target_encoding(X_test, encoder, encoded_columns)\n",
    "        X_test = cast_numeric_dtypes(X_test)\n",
    "        X_test_arr = X_test.to_pandas()\n",
    "        \n",
    "        # Add predictions\n",
    "        fold_pred = model.predict(X_test_arr)\n",
    "        test_predictions += fold_pred\n",
    "        \n",
    "        # Log fold performance\n",
    "        val_score = model.best_score_[\"valid_1\"][cfg.metric]\n",
    "        print(f\"Fold {fold+1} validation score: {val_score}\")\n",
    "        wandb.log({f\"fold_{fold+1}_val_score\": val_score})\n",
    "        \n",
    "        # Cleanup\n",
    "        gc.collect()\n",
    "    \n",
    "    return test_predictions\n",
    "\n",
    "\n",
    "def target_encode_polars(X_train, y_train, X_valid, columns, random_state=42):\n",
    "\n",
    "    # Initialize the encoder\n",
    "    encoder = TargetEncoder(random_state=random_state)\n",
    "    \n",
    "    # We'll need numpy arrays for the encoding\n",
    "    X_train_numpy = X_train.select(columns).to_numpy()\n",
    "    y_train_numpy = y_train.to_numpy()\n",
    "    X_valid_numpy = X_valid.select(columns).to_numpy()\n",
    "    \n",
    "    # Fit and transform\n",
    "    X_train_encoded = encoder.fit_transform(X_train_numpy, y_train_numpy)\n",
    "    X_valid_encoded = encoder.transform(X_valid_numpy)\n",
    "    \n",
    "    # Create Polars DataFrames from the encoded results\n",
    "    encoded_train_df = pl.DataFrame(\n",
    "        {col: X_train_encoded[:, i] for i, col in enumerate(columns)}\n",
    "    )\n",
    "    encoded_valid_df = pl.DataFrame(\n",
    "        {col: X_valid_encoded[:, i] for i, col in enumerate(columns)}\n",
    "    )\n",
    "    \n",
    "    # Drop the original columns and add the encoded ones\n",
    "    X_train = X_train.drop(columns)\n",
    "    X_valid = X_valid.drop(columns)\n",
    "    \n",
    "    # Horizontal stack with encoded columns\n",
    "    X_train = pl.concat([X_train, encoded_train_df], how=\"horizontal\")\n",
    "    X_valid = pl.concat([X_valid, encoded_valid_df], how=\"horizontal\")\n",
    "    \n",
    "    return X_train, X_valid, encoder\n",
    "\n",
    "def apply_target_encoding(X_test, encoder, columns):\n",
    "    X_test_numpy = X_test.select(columns).to_numpy()\n",
    "    X_test_encoded = encoder.transform(X_test_numpy)\n",
    "    \n",
    "    encoded_test_df = pl.DataFrame(\n",
    "        {col: X_test_encoded[:, i] for i, col in enumerate(columns)}\n",
    "    )\n",
    "    X_test = X_test.drop(columns)\n",
    "    X_test = pl.concat([X_test, encoded_test_df], how=\"horizontal\")\n",
    "    \n",
    "    return X_test\n",
    "    \n",
    "\n",
    "# Initialize wandb\n",
    "user_secrets = UserSecretsClient()\n",
    "WANDB_API_KEY = user_secrets.get_secret(\"wandb_api\")\n",
    "wandb.login(key=WANDB_API_KEY)\n",
    "wandb_run = wandb.init(project=\"playground-series-s5e4\", config=asdict(cfg))\n",
    "\n",
    "# Run the KFold training process\n",
    "test_preds = get_dfs_with_kfold(cfg)\n",
    "\n",
    "# Log final results\n",
    "wandb.log({\"final_predictions\": wandb.Histogram(test_preds)})\n",
    "wandb.finish()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7cf97d6c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T13:23:04.054570Z",
     "iopub.status.busy": "2025-04-21T13:23:04.053915Z",
     "iopub.status.idle": "2025-04-21T13:23:04.652018Z",
     "shell.execute_reply": "2025-04-21T13:23:04.651345Z"
    },
    "papermill": {
     "duration": 0.694701,
     "end_time": "2025-04-21T13:23:04.653260",
     "exception": false,
     "start_time": "2025-04-21T13:23:03.958559",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Listening_Time_minutes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>750000</td>\n",
       "      <td>53.905358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>750001</td>\n",
       "      <td>22.425277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>750002</td>\n",
       "      <td>48.466611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>750003</td>\n",
       "      <td>78.328807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>750004</td>\n",
       "      <td>52.016963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249995</th>\n",
       "      <td>999995</td>\n",
       "      <td>10.687201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249996</th>\n",
       "      <td>999996</td>\n",
       "      <td>55.222936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249997</th>\n",
       "      <td>999997</td>\n",
       "      <td>6.158897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249998</th>\n",
       "      <td>999998</td>\n",
       "      <td>75.895708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249999</th>\n",
       "      <td>999999</td>\n",
       "      <td>57.595637</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>250000 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            id  Listening_Time_minutes\n",
       "0       750000               53.905358\n",
       "1       750001               22.425277\n",
       "2       750002               48.466611\n",
       "3       750003               78.328807\n",
       "4       750004               52.016963\n",
       "...        ...                     ...\n",
       "249995  999995               10.687201\n",
       "249996  999996               55.222936\n",
       "249997  999997                6.158897\n",
       "249998  999998               75.895708\n",
       "249999  999999               57.595637\n",
       "\n",
       "[250000 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import display\n",
    "import pandas as pd\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "df_subm = pd.read_csv(cfg.sub_path)\n",
    "df_subm[\"Listening_Time_minutes\"] = test_preds / cfg.num_fold\n",
    "df_subm.to_csv('raw_submission.csv', index=False)\n",
    "df_subm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd06b331",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T13:23:04.907495Z",
     "iopub.status.busy": "2025-04-21T13:23:04.906847Z",
     "iopub.status.idle": "2025-04-21T13:23:07.177200Z",
     "shell.execute_reply": "2025-04-21T13:23:07.176576Z"
    },
    "papermill": {
     "duration": 2.431123,
     "end_time": "2025-04-21T13:23:07.178549",
     "exception": false,
     "start_time": "2025-04-21T13:23:04.747426",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('/kaggle/input/playground-series-s5e4/train.csv')\n",
    "df_test = pd.read_csv('/kaggle/input/playground-series-s5e4/test.csv')\n",
    "\n",
    "df_pltpd = pd.read_csv('/kaggle/input/podcast-listening-time-prediction-dataset/podcast_dataset.csv')\n",
    "df_pltpd = df_pltpd.dropna(subset=[\"Listening_Time_minutes\"])\n",
    "df_pltpd = df_pltpd.reset_index(drop=True)\n",
    "df_pltpd.index = df_pltpd.index + 1000000\n",
    "df_pltpd['id'] = df_pltpd.index\n",
    "\n",
    "df_train = pd.concat([df_train, df_pltpd], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1280e9b3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T13:23:07.365046Z",
     "iopub.status.busy": "2025-04-21T13:23:07.364456Z",
     "iopub.status.idle": "2025-04-21T13:23:07.406942Z",
     "shell.execute_reply": "2025-04-21T13:23:07.406353Z"
    },
    "papermill": {
     "duration": 0.136761,
     "end_time": "2025-04-21T13:23:07.408046",
     "exception": false,
     "start_time": "2025-04-21T13:23:07.271285",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Podcast_Name</th>\n",
       "      <th>Episode_Title</th>\n",
       "      <th>Episode_Length_minutes</th>\n",
       "      <th>Genre</th>\n",
       "      <th>Host_Popularity_percentage</th>\n",
       "      <th>Publication_Day</th>\n",
       "      <th>Publication_Time</th>\n",
       "      <th>Guest_Popularity_percentage</th>\n",
       "      <th>Number_of_Ads</th>\n",
       "      <th>Episode_Sentiment</th>\n",
       "      <th>Listening_Time_minutes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>37939</th>\n",
       "      <td>787939</td>\n",
       "      <td>Life Lessons</td>\n",
       "      <td>Episode 94</td>\n",
       "      <td>89.84</td>\n",
       "      <td>Lifestyle</td>\n",
       "      <td>60.79</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>Afternoon</td>\n",
       "      <td>62.13</td>\n",
       "      <td>89.12</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>88.49616</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id  Podcast_Name Episode_Title  Episode_Length_minutes      Genre  \\\n",
       "37939  787939  Life Lessons    Episode 94                   89.84  Lifestyle   \n",
       "\n",
       "       Host_Popularity_percentage Publication_Day Publication_Time  \\\n",
       "37939                       60.79          Sunday        Afternoon   \n",
       "\n",
       "       Guest_Popularity_percentage  Number_of_Ads Episode_Sentiment  \\\n",
       "37939                        62.13          89.12           Neutral   \n",
       "\n",
       "       Listening_Time_minutes  \n",
       "37939                88.49616  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Listening_Time_minutes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>37939</th>\n",
       "      <td>787939</td>\n",
       "      <td>62.470018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id  Listening_Time_minutes\n",
       "37939  787939               62.470018"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Listening_Time_minutes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>37939</th>\n",
       "      <td>787939</td>\n",
       "      <td>88.49616</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id  Listening_Time_minutes\n",
       "37939  787939                88.49616"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_test_with_id = df_test.copy()\n",
    "\n",
    "df_test_with_id.loc[df_test_with_id[\"Number_of_Ads\"] > 103.91, \"Number_of_Ads\"] = 0.0\n",
    "df_over_ads = df_test_with_id[df_test_with_id[\"Number_of_Ads\"] > 3]\n",
    "df_over_ads[\"Listening_Time_minutes\"] = df_over_ads[\"Number_of_Ads\"] * 0.993\n",
    "display(df_over_ads)\n",
    "\n",
    "display(df_subm.loc[df_subm['id'].isin(df_over_ads['id'].values)])\n",
    "df_subm.loc[df_subm['id'].isin(df_over_ads['id'].values), 'Listening_Time_minutes'] = df_over_ads['Listening_Time_minutes'].values\n",
    "display(df_subm.loc[df_subm['id'].isin(df_over_ads['id'].values)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "23bbf575",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T13:23:07.596281Z",
     "iopub.status.busy": "2025-04-21T13:23:07.595564Z",
     "iopub.status.idle": "2025-04-21T13:23:08.043280Z",
     "shell.execute_reply": "2025-04-21T13:23:08.042612Z"
    },
    "papermill": {
     "duration": 0.542254,
     "end_time": "2025-04-21T13:23:08.044432",
     "exception": false,
     "start_time": "2025-04-21T13:23:07.502178",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Listening_Time_minutes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>750051</td>\n",
       "      <td>18.481845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>750052</td>\n",
       "      <td>60.222327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>750227</td>\n",
       "      <td>63.558446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243</th>\n",
       "      <td>750243</td>\n",
       "      <td>23.547206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>312</th>\n",
       "      <td>750312</td>\n",
       "      <td>46.616548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249802</th>\n",
       "      <td>999802</td>\n",
       "      <td>38.425203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249832</th>\n",
       "      <td>999832</td>\n",
       "      <td>85.600043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249841</th>\n",
       "      <td>999841</td>\n",
       "      <td>19.826841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249854</th>\n",
       "      <td>999854</td>\n",
       "      <td>69.605168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249876</th>\n",
       "      <td>999876</td>\n",
       "      <td>19.734575</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9498 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            id  Listening_Time_minutes\n",
       "51      750051               18.481845\n",
       "52      750052               60.222327\n",
       "227     750227               63.558446\n",
       "243     750243               23.547206\n",
       "312     750312               46.616548\n",
       "...        ...                     ...\n",
       "249802  999802               38.425203\n",
       "249832  999832               85.600043\n",
       "249841  999841               19.826841\n",
       "249854  999854               69.605168\n",
       "249876  999876               19.734575\n",
       "\n",
       "[9498 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Listening_Time_minutes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>750051</td>\n",
       "      <td>18.060370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>750052</td>\n",
       "      <td>65.789562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>750227</td>\n",
       "      <td>58.393200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243</th>\n",
       "      <td>750243</td>\n",
       "      <td>23.830000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>312</th>\n",
       "      <td>750312</td>\n",
       "      <td>26.825190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249802</th>\n",
       "      <td>999802</td>\n",
       "      <td>45.055790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249832</th>\n",
       "      <td>999832</td>\n",
       "      <td>89.819210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249841</th>\n",
       "      <td>999841</td>\n",
       "      <td>17.234691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249854</th>\n",
       "      <td>999854</td>\n",
       "      <td>67.816132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249876</th>\n",
       "      <td>999876</td>\n",
       "      <td>19.197310</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9498 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            id  Listening_Time_minutes\n",
       "51      750051               18.060370\n",
       "52      750052               65.789562\n",
       "227     750227               58.393200\n",
       "243     750243               23.830000\n",
       "312     750312               26.825190\n",
       "...        ...                     ...\n",
       "249802  999802               45.055790\n",
       "249832  999832               89.819210\n",
       "249841  999841               17.234691\n",
       "249854  999854               67.816132\n",
       "249876  999876               19.197310\n",
       "\n",
       "[9498 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cols_to_compare = ['Episode_Title', 'Host_Popularity_percentage', 'Guest_Popularity_percentage']\n",
    "\n",
    "df_test_with_id = df_test.copy()\n",
    "df_test_with_id = df_test_with_id.dropna(subset=['Guest_Popularity_percentage'])\n",
    "\n",
    "leaked_rows = df_test_with_id.merge(\n",
    "    df_train[cols_to_compare + ['Listening_Time_minutes']].drop_duplicates(),\n",
    "    on=cols_to_compare,\n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "mean_values = leaked_rows.groupby('id')['Listening_Time_minutes'].mean().reset_index()\n",
    "\n",
    "display(df_subm.loc[df_subm['id'].isin(mean_values['id'].values)])\n",
    "df_subm.loc[df_subm['id'].isin(mean_values['id'].values), \"Listening_Time_minutes\"] = mean_values[\"Listening_Time_minutes\"].values\n",
    "display(df_subm.loc[df_subm['id'].isin(mean_values['id'].values)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0ecca02b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T13:23:08.232681Z",
     "iopub.status.busy": "2025-04-21T13:23:08.232418Z",
     "iopub.status.idle": "2025-04-21T13:23:08.749521Z",
     "shell.execute_reply": "2025-04-21T13:23:08.748892Z"
    },
    "papermill": {
     "duration": 0.612482,
     "end_time": "2025-04-21T13:23:08.750703",
     "exception": false,
     "start_time": "2025-04-21T13:23:08.138221",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (250_000, 2)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>id</th><th>Listening_Time_minutes</th></tr><tr><td>i64</td><td>f64</td></tr></thead><tbody><tr><td>750000</td><td>53.905358</td></tr><tr><td>750001</td><td>22.425277</td></tr><tr><td>750002</td><td>48.466611</td></tr><tr><td>750003</td><td>78.328807</td></tr><tr><td>750004</td><td>52.016963</td></tr><tr><td>&hellip;</td><td>&hellip;</td></tr><tr><td>999995</td><td>10.687201</td></tr><tr><td>999996</td><td>55.222936</td></tr><tr><td>999997</td><td>6.158897</td></tr><tr><td>999998</td><td>75.895708</td></tr><tr><td>999999</td><td>57.595637</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (250_000, 2)\n",
       "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
       "‚îÇ id     ‚îÜ Listening_Time_minutes ‚îÇ\n",
       "‚îÇ ---    ‚îÜ ---                    ‚îÇ\n",
       "‚îÇ i64    ‚îÜ f64                    ‚îÇ\n",
       "‚ïû‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï°\n",
       "‚îÇ 750000 ‚îÜ 53.905358              ‚îÇ\n",
       "‚îÇ 750001 ‚îÜ 22.425277              ‚îÇ\n",
       "‚îÇ 750002 ‚îÜ 48.466611              ‚îÇ\n",
       "‚îÇ 750003 ‚îÜ 78.328807              ‚îÇ\n",
       "‚îÇ 750004 ‚îÜ 52.016963              ‚îÇ\n",
       "‚îÇ ‚Ä¶      ‚îÜ ‚Ä¶                      ‚îÇ\n",
       "‚îÇ 999995 ‚îÜ 10.687201              ‚îÇ\n",
       "‚îÇ 999996 ‚îÜ 55.222936              ‚îÇ\n",
       "‚îÇ 999997 ‚îÜ 6.158897               ‚îÇ\n",
       "‚îÇ 999998 ‚îÜ 75.895708              ‚îÇ\n",
       "‚îÇ 999999 ‚îÜ 57.595637              ‚îÇ\n",
       "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_subm.to_csv('submission.csv', index=False)\n",
    "pl.read_csv('submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f1bec9",
   "metadata": {
    "papermill": {
     "duration": 0.091579,
     "end_time": "2025-04-21T13:23:08.935606",
     "exception": false,
     "start_time": "2025-04-21T13:23:08.844027",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913dfad5",
   "metadata": {
    "papermill": {
     "duration": 0.093315,
     "end_time": "2025-04-21T13:23:09.123077",
     "exception": false,
     "start_time": "2025-04-21T13:23:09.029762",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 11351736,
     "sourceId": 91715,
     "sourceType": "competition"
    },
    {
     "datasetId": 5656615,
     "sourceId": 9335009,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31012,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 6434.66959,
   "end_time": "2025-04-21T13:23:12.622455",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-04-21T11:35:57.952865",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
